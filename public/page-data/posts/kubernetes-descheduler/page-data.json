{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/kubernetes-descheduler","result":{"data":{"markdownRemark":{"id":"34ed108b-9e31-5177-9193-ffacb7767c79","html":"<h2 id=\"why-do-we-need-balanced-scheduling\" style=\"position:relative;\"><a href=\"#why-do-we-need-balanced-scheduling\" aria-label=\"why do we need balanced scheduling permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why do we need balanced scheduling?</h2>\n<p>Suppose we have four machines in our kubernetes cluster available to run our tasks. </p>\n<p>There are two kinds of scheduling strategies involved, (a) compact and (b) dispersed. A compact strategy would always allocate tasks to the first machine, until it runs out of resources. A scheduler focussed on dispersion would judge the resource requirements, and fairly distribute it among the different machines (this is balanced scheduling).</p>\n<p>Both scheduling strategies aim to achieve different types of goals. A compact strategy will improve overall machine utilisation rates, and also allow for improved performance if processes are communicating locally. On the other hand, a balanced strategy would mean that machines are running with some CPU buffer available, allowing them to better handle burst traffic and I/O requests. </p>\n<p>By default, kubernetes comes with its own <code class=\"language-text\">kube-scheduler</code>, though we can define our own scheduler service to overwrite it.</p>\n<h2 id=\"resource-specification-requests-vs-limits\" style=\"position:relative;\"><a href=\"#resource-specification-requests-vs-limits\" aria-label=\"resource specification requests vs limits permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Resource Specification: Requests vs Limits</h2>\n<p>In kubernetes, resources are defined with two fields - requests and limits. Every resource type with have both a request and limit, so it can be extended from CPU to memory, GPU and even sockets. These values are defined by the users themselves, and provide the kube-scheduler with an indication of how much resources the task requires.</p>\n<p><strong>Limit</strong>. This is the maximum amount of resources the service can consume, exceeding which it will be killed. We can think of this as doing a hard limit (<code class=\"language-text\">prlimit</code>) on the main process of the service.</p>\n<p><strong>Request</strong>. This is the minimum amount of resources the service must be guaranteed. That is, if a machine has less <em>available</em> resources that what is requested, the service will not be scheduled to that machine. It is a baseline quality of service.</p>\n<h4 id=\"why-not-just-set-request--limit\" style=\"position:relative;\"><a href=\"#why-not-just-set-request--limit\" aria-label=\"why not just set request  limit permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Why not just set <code class=\"language-text\">request == limit</code>?</h4>\n<p>Oftentimes, many services like web servers are idle unless the service is triggered, so the idle and running usage could differ significantly. When we set a high request value, we have to guarantee machines have resources available, and will quickly run out of machines. This depletion could happen even if many machines are almost idle, since the services are not actively serving users.</p>\n<h2 id=\"using-a-descheduler\" style=\"position:relative;\"><a href=\"#using-a-descheduler\" aria-label=\"using a descheduler permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Using a Descheduler</h2>\n<p>By introducing the concept of resource requests, we bring about a new set of challenges. Namely, suppose that our cluster of four machines is now mostly idle. It could happen that two services scheduled on the exact same machine #1 both require heavy resources at the same time. For instance, both are trying to perform I/O operations like writing to the disk, or both are competing for network resources. </p>\n<p>At this point in time, one machine in the cluster is exceptionally overloaded, while the other three machines continue to remain fairly idle. It is hard to avoid such a scenario entirely, unless we have some knowledge on when each service will have burst traffic. We need to then perform some <strong>rebalancing</strong> of the services, such as moving one of the burst traffic services to an idle machine.</p>\n<p>This can be achieved in kubernetes by <strong>evicting</strong> the pod of the service. When the pod is evicted, the scheduler will have to again decide where to place this pod. Based on the current machine resource usage (i.e. machine #1 being overworked), the pod will be preferentially scheduled to one of the three idle machines. The net effect is that the machine utilisations are more balanced.</p>\n<h4 id=\"when-should-we-perform-eviction\" style=\"position:relative;\"><a href=\"#when-should-we-perform-eviction\" aria-label=\"when should we perform eviction permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>When should we perform eviction?</h4>\n<p>Evicting pods causes the whole container creation process to restart, meaning it should be avoided as much as possible. Furthermore, if every machine in the cluster is already overworked, we should not perform any eviction.</p>\n<p>An eviction policy might then look like this:</p>\n<p><strong>High Water Level</strong>. Start eviction if a machine is overworked past a certain high water level.</p>\n<p><strong>Significant Difference</strong>. Start eviction only if there is a significant difference between two machines’ utilisations. If all machines fall within a utilisation range of say 40% to 65%, we might not want to evict anything even if the machine running at 65% is slightly overworked.</p>\n<h4 id=\"how-do-we-decide-what-services-to-evict\" style=\"position:relative;\"><a href=\"#how-do-we-decide-what-services-to-evict\" aria-label=\"how do we decide what services to evict permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>How do we decide what services to evict?</h4>\n<p>If there are many services running on a machine, we would preferentially remove these types of services:</p>\n<ul>\n<li><strong>High Usage</strong>. Evicting a high-usage service guarantees an instant relief on the machine resources.</li>\n<li><strong>Many Replicas</strong>. Even when evicted, the service still remains available for users, reducing the impact of eviction.</li>\n</ul>\n<p>However, there are edge cases on eviction that should be reviewed:</p>\n<ul>\n<li><strong>No reliance on machine</strong>: Certain machines have specific label selectors for services, or contain local data that the service needs. Even if we evict such services, the scheduler might put them back on the same machine.</li>\n<li><strong>Small and medium services only</strong>. Some services use a disproportionately large amounts of resources, say 80% of an entire machine. There is limited value in eviction, because re-scheduling the service merely causes it to solely consume another machine’s resources.</li>\n<li><strong>Ignore socket-only machines</strong>. Socket services reserve a set of the machine’s resources. If the machine only supports socket services, then one service’s high usage will not throttle another service on the same machine.</li>\n</ul>\n<h2 id=\"remarks\" style=\"position:relative;\"><a href=\"#remarks\" aria-label=\"remarks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Remarks</h2>\n<p>Scheduling and de-scheduling dynamically changing services is a complex affair. The descheduler, while able to achieve balanced scheduling, is not suitable for every scenario and also has the additional downside of restarting services. As usual, we should be discerning in when to adopt such a solution in production.</p>\n<p>The descheduler project by kubernetes-sigs can be found <a href=\"https://github.com/kubernetes-sigs/descheduler\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">here</a>.</p>","fields":{"slug":"/posts/kubernetes-descheduler","tagSlugs":["/tag/kubernetes/","/tag/descheduler/"]},"frontmatter":{"date":"2023-08-24","description":"Use a de-scheduler to achieve balanced scheduling","tags":["kubernetes","descheduler"],"title":"Balanced Scheduling in Kubernetes"}}},"pageContext":{"slug":"/posts/kubernetes-descheduler"}},"staticQueryHashes":["251939775","3439816877","401334301"]}