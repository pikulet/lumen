{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/gambling-info-theory","result":{"data":{"markdownRemark":{"id":"d5d52e84-6ecb-535f-8b60-c82d187d1c34","html":"<h3 id=\"information-theory\" style=\"position:relative;\"><a href=\"#information-theory\" aria-label=\"information theory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Information Theory</h3>\n<p>Information Theory is a field concerned with how much information certain events can reveal. In general, events with a lower probability have more information. The commonly-used measure of “entropy” refers to the average amount of information across all events. </p>\n<p>If variable <code class=\"language-text\">A</code> can hold the values of <code class=\"language-text\">a1</code> with probability <code class=\"language-text\">0</code> and <code class=\"language-text\">a2</code> with probability <code class=\"language-text\">1</code>, then we know with absolute certainty that variable <code class=\"language-text\">A</code> will take on the value <code class=\"language-text\">a2</code>. Hence, the entropy of this variable is zero.</p>\n<p>On the other hand, if variable <code class=\"language-text\">B</code> can be <code class=\"language-text\">b1</code> or <code class=\"language-text\">b2</code> with equal probability, then it would have the highest average entropy. In essence, entropy is a measure of the amount of uncertainty in a system.</p>\n<h3 id=\"usefulness-of-information-theory\" style=\"position:relative;\"><a href=\"#usefulness-of-information-theory\" aria-label=\"usefulness of information theory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Usefulness of Information Theory</h3>\n<p>Information Theory is relevant to a lot of fields, but I will explain two bigs ones - Machine Learning and Cryptography.</p>\n<h4 id=\"machine-learning\" style=\"position:relative;\"><a href=\"#machine-learning\" aria-label=\"machine learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Machine Learning</h4>\n<p>Consider an algorithm that aims to classify if an image is a cat or dog. The result is stored in the random variable X. Without any prior information, the amount of information in X is exactly the same as the distribution of cat and dog images on the internet. </p>\n<p>However, if we have some training data, then we can instead consider the conditional entropy of the prediction outcome X given the data. An algorithm that works well would aim to minimise this entropy in X using more relevant training data and methods.</p>\n<p>By measuring the uncertainty in the prediction outcome X, we can better devise algorithms that aim to reduce the uncertainty in prediction models.</p>\n<h4 id=\"cryptography\" style=\"position:relative;\"><a href=\"#cryptography\" aria-label=\"cryptography permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Cryptography</h4>\n<p>Encoding and decoding bits is at the core of information theory. Given a message like “hello, welcome to this site”. How many bits do we need to encode it such that</p>\n<ul>\n<li>the receiver can correctly decode the message</li>\n<li>the message is resistant to errors in the channel, such as incorrectly-transmitted Morse code?</li>\n</ul>\n<p>The answer depends on the entropy of the language of transmission, e.g. English. One letter in English contains less information than one character in Japanese. Compression algorithms might rely on some information about English to make the message consume fewer bits, like <code class=\"language-text\">hiwelcome2site</code>.</p>\n<p>English speakers are able to effectively decode the meaning of the sentence without punctuation and with some spelling variations. Even if a few letters are lost in transmission, we might have a good chance of still understanding the overall message.</p>\n<p>An interesting use case of language compression is in <em>stenography</em>, where a few buttons can be used to effectively communicate the English language. By studying Information Theory, we can prove the fundamental limits in what can be achieved in real-world encoding of text, images and even audio.</p>\n<h3 id=\"the-project\" style=\"position:relative;\"><a href=\"#the-project\" aria-label=\"the project permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>The Project</h3>\n<p>For my term project in the module, I did a report on Gambling and Data Compression, from Cover and Thomas’ chapter. The authors were using horse races for English text prediction.</p>\n<p>In short, an information-theoretic perspective was used to frame the horse race gambling problem. The results from this analysis were then applied in the prediction of English language text. </p>\n<p>The predictor is basically a horse better, making bets on the next letter that will occur in a given sequence of letters. Hence, if the gambler can make good predictions and earn a lot of wealth, the predictor can also make good predictions for the english language.</p>\n<p>The report is targeted at readers familiar with basic notations in Information Theory. If you’d like to learn more about the topic, I can direct you to some great resources. Interested readers can read the full analysis <a href=\"/gambling.pdf\">here</a>.</p>","fields":{"slug":"/posts/gambling-info-theory","tagSlugs":["/tag/information-theory/"]},"frontmatter":{"date":"2020-04-30","description":"Using an information-theoretic perspective to understand horse races","tags":["information theory"],"title":"Gambling and Data Compression"}}},"pageContext":{"slug":"/posts/gambling-info-theory"}},"staticQueryHashes":["251939775","3439816877","401334301"]}