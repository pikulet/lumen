{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/dsta-til-hack","result":{"data":{"markdownRemark":{"id":"39be3437-ab28-5640-bebd-5d380ba9ba1d","html":"<p>Together with some friends, I participated in the 2019 DSTA-TIL-AI camp. This AI camp is held in conjunction with the more well-known Cyber Defenders Discovery Camp (CDDC), a Capture-the-Flag (CTF) competition for computer security enthusiasts.</p>\n<p>The challenge was to train an image classifier to identify poses people were doing. This problem is known as <strong>pose estimation</strong>.</p>\n<h3 id=\"image-classification\" style=\"position:relative;\"><a href=\"#image-classification\" aria-label=\"image classification permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Classification</h3>\n<p>In the world of machine learning, image classification is best done using <a href=\"https://medium.com/@ksusorokina/image-classification-with-convolutional-neural-networks-496815db12a8\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Convolutional Neural Networks (CNNs)</a>. These neural networks are able to extract features like lines, and gradually build lines into shapes (e.g. circles), and eventually recognisable features (e.g. faces). </p>\n<p>This <a href=\"https://towardsdatascience.com/intuitively-understanding-convolutions-for-deep-learning-1f6f42faee1\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">article</a> gives a great explanation of convolutions. It is a must-read for anyone new to convolutions.</p>\n<h3 id=\"existing-pose-estimators\" style=\"position:relative;\"><a href=\"#existing-pose-estimators\" aria-label=\"existing pose estimators permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Existing Pose Estimators</h3>\n<p>We thought of using existing neural networks that could extract features, such as OpenPose. Our idea was to use OpenPose to extract the coordinates of the joints, and then use these coordinates to train our model instead.</p>\n<p>However, there were many limitations in applying this to our hackathon. For one, some of our poses were hand signals (e.g. spiderman fingers and gun poses). Given the joint-recognition technology, there was no clear way to distinguish these two poses.</p>\n<p>Furthermore, these models were not sufficiently well-trained. Then, we would be diminishing the data from the training images into (possibly inaccurate) coordinates.</p>\n<h3 id=\"transfer-learning\" style=\"position:relative;\"><a href=\"#transfer-learning\" aria-label=\"transfer learning permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Transfer Learning</h3>\n<p>What we did then, was to instead re-train an existing neural network using transfer learning. The idea is to let the model fit better to the set of images we were working with.</p>\n<p>Imagine a neural network with multiple layers. Transfer learning usually unfreezes the last few layers of the neural network, and train them. The first few layers remain frozen as these are extracting basic features (e.g. edge detection).</p>\n<p><code class=\"language-text\">keras</code> has many models available for transfer learning, such as Xception and Inception.</p>\n<h3 id=\"data-augmentation\" style=\"position:relative;\"><a href=\"#data-augmentation\" aria-label=\"data augmentation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Augmentation</h3>\n<p>Lastly, images can vary even if they are depicting the same object. Data augmentation is vital in image classification. The basic data augmentation that keras can perform includes rotating, translating and resizing images.</p>\n<p>The top-performing team did very interesting work in data augmentation, which Iâ€™m truly impressed with.</p>\n<ul>\n<li>They cropped the humans out, and applied different background colours. This technique draws more focus on the shape of the human, effectively dulling background noise.</li>\n<li>They took more photos of themselves in the poses, at different angles. Adding more data was a whole lot of fun for them, and also greatly improved their training data.</li>\n</ul>\n<h3 id=\"google-colaboratory\" style=\"position:relative;\"><a href=\"#google-colaboratory\" aria-label=\"google colaboratory permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Google Colaboratory</h3>\n<p>While we were given AWS credits for this hackathon, we found it more useful to use Google Colab. The training speed was about the same, and <a href=\"https://colab.research.google.com/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">Google Colab</a> offered more convenience. </p>\n<p>Jupyter notebooks can be imported and run. Google Colab is really useful for anyone new to get started with machine learning without all the hassle of AWS.</p>\n<h3 id=\"reflections\" style=\"position:relative;\"><a href=\"#reflections\" aria-label=\"reflections permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reflections</h3>\n<p>Pose estimation has many real-world uses. What I would really like to see is a sign-language interpreter, which would help bridge the gap between the signing and speaking worlds.</p>\n<h3 id=\"remarks\" style=\"position:relative;\"><a href=\"#remarks\" aria-label=\"remarks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Remarks</h3>\n<p>A quick summary can be found in this <a href=\"/media/toothtable-ppt.pdf\">presentation deck</a>.</p>","fields":{"slug":"/posts/dsta-til-hack","tagSlugs":["/tag/pose-estimator/","/tag/google-collab/"]},"frontmatter":{"date":"2019-06-26","description":"Training an image classfier for to estimate people's poses","tags":["pose estimator","google collab"],"title":"DSTA Today-I-Learned Artificial Intelligence Camp"}}},"pageContext":{"slug":"/posts/dsta-til-hack"}},"staticQueryHashes":["251939775","3439816877","401334301"]}