{"componentChunkName":"component---src-templates-post-template-js","path":"/posts/poker-agent","result":{"data":{"markdownRemark":{"id":"853c2f9f-7bf9-5cf7-8605-3712e7870fef","html":"<p>In a module on Artificial Intelligence, we were tasked with building a poker agent playing a simplified form of limit poker. The poker engine is found <a href=\"https://github.com/ishikota/PyPokerEngine\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">here</a>.</p>\n<p>Basically, the agent has a function to decide which action to take (raise, call or fold) at every turn in poker. The agent has access to the game state, such as the amount of money in the pot, the current hands and most importantly, the previous actions taken by the opponent.</p>\n<p>Given the large complexity of poker, we had to apply abstractions to make it feasible to develop a poker agent. I’ll be sharing some of our insights into the game, and the techniques used.</p>\n<h4 id=\"abstracting-game-states\" style=\"position:relative;\"><a href=\"#abstracting-game-states\" aria-label=\"abstracting game states permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstracting Game States</h4>\n<blockquote>\n<p>This section deals with how we can simplify the model of the poker game. Abstraction and simplification often leads to some loss of data accuracy. However, as mentioned, it is more important to make big-picture decisions first.</p>\n</blockquote>\n<p><strong>Card Isomorphism</strong>: In poker, the suits of the card do not matter much. Having a King of Hearts with Four of Clubs is almost the same as having a King of Spades and a Four of Diamonds. Hence, the initial hand size of two can be stored in a table of size <code class=\"language-text\">13 x 13 = 169</code> instead of <code class=\"language-text\">50 choose 2 = 1326</code>.</p>\n<p><img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Sklansky_Texas_Holdem_Starting_Hand_Strategies.JPG\" alt=\"poker states\"></p>\n<p>Card isomorphism is demonstrated in the photo above. A suited hand means that both cards belong to the same suit, offering a higher chance of completing a Flush.</p>\n<p><strong>Card Bucketing</strong>: Card isomorphism is insufficient to reduce the size of the game state space, so card bucketing is used. </p>\n<p>Instead of considering the many possible hands, we simply considered 5 buckets of cards (Very Strong, Strong, Average, Weak, Very Weak). Again, it does not really matter whether you have two Jacks and a seven or two Queens and a five. </p>\n<p>We used card win rate as the bucket key. Given a hand, we can estimate its winrate. We then allocate it a bucket depending on the winrate value.</p>\n<p>Generating good card buckets is a topic of research that takes a lot of time to train. We circumvented this training time using a Monte Carlo approach to generate the bucket margins. In essence, we ran many random experiments such that the experimental data can give reasonably valuable numerical outcomes.</p>\n<p>We randomly generated 1000 possible hands. For each hand, we ran them through a Monte Carlo simulation to retrieve a win rate value. We then calculate the win rate at the 20th, 40th, 60th and 80th percentiles. These were the values used to demarcate the buckets.</p>\n<h4 id=\"abstracting-player-behaviour\" style=\"position:relative;\"><a href=\"#abstracting-player-behaviour\" aria-label=\"abstracting player behaviour permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Abstracting Player Behaviour</h4>\n<blockquote>\n<p>This section aims to model player strategies in poker.</p>\n</blockquote>\n<p><strong>Aggressiveness</strong>: Given a strong hand, how much do you want to raise, or just call?</p>\n<ul>\n<li>Raising too aggressively hints to opponents that your hand is very strong, causing them to fold early (and reducing the pot size). This bluff technique aims to let opponents underestimate your hand strength.</li>\n</ul>\n<p><strong>Tightness</strong>: Given a weak hand, how much risk do you want to take, to raise/ call instead of folding?</p>\n<ul>\n<li>This bluff technique aims to let opponents overestimate your hand strength.</li>\n</ul>\n<p>Every player has a different level of aggressiveness and tightness, and the poker agent must be able to guess the opponent’s play style. We used the play sequence (call, call, raise etc) to aggregate these attributes.</p>\n<h4 id=\"importance-of-random-behaviour\" style=\"position:relative;\"><a href=\"#importance-of-random-behaviour\" aria-label=\"importance of random behaviour permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Importance of Random Behaviour</h4>\n<p>A predictable player would be easily countered. Instead of having determinate actions to take, our agent has a 3-tuple of probability that each action (raise, call, fold) would be played.</p>\n<p>For instance, (0.8, 0.1, 0.1) means that the agent raises with probability 80%, calls with probability 10% and folds with probability 10%. With the exact same hand, our poker agent would not take the same action every time. This strategy is especially important with just two cards on hand, since there are only <code class=\"language-text\">169</code> states after applying card isomorphism. With sufficient games, it is not hard to predict a poker agent’s behaviour for each of those states. For instance, we might not always fold even with the weakest possible hand. Similarly, we might not always raise with the strongest possible hand (we could just call).</p>\n<h4 id=\"aggregating-the-data-into-an-agent\" style=\"position:relative;\"><a href=\"#aggregating-the-data-into-an-agent\" aria-label=\"aggregating the data into an agent permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Aggregating the Data into an Agent</h4>\n<p>The simplest agent would just put all these values into a linear combination and optimise the weights. We were adventurous and decided to try out two learning techniques - Counter-Factual Regret Minimisation (CFR Method) and Deep-Q Network Learning (DQN).</p>\n<p>A CFR agent basically plays multiple games, and observes its regret for different actions. That is, given it had a bad hand, raised, and lost, it will regret raising at a bad hand. The agent then adjusts the probability for the actions, lowering the raise probability and increasing the fold probability.</p>\n<p>In the long term, the agent wants to minimise the regret across all actions.\nHowever, CFR agents take very long to train. There are multiple game states and it takes too long to reach the equilibrium state.</p>\n<p>We then turned to the Deep-Q Network methodology. The paradigm of DQNs parallel CFR agents: each action has an output value mapped to it. The results take a much faster time to approach equilibrium, which was favourable given our lack of training resources.</p>\n<h4 id=\"conclusion\" style=\"position:relative;\"><a href=\"#conclusion\" aria-label=\"conclusion permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conclusion</h4>\n<p>Developing a complex poker agent was definitely fun, and let me understand more about artificial intelligence. What does it really mean for a machine to be able to make decisions? What are the implications on human society?</p>\n<p>A small aside, suppose that we managed to train and agent that could win humans at poker 90% of the time (we didn’t actually achieve this feat). Does that AI <em>know</em> how to play poker? This thought is precisely the <em>Chinese Room Argument</em> in the philosophy of artificial intelligence.</p>\n<h4 id=\"remarks\" style=\"position:relative;\"><a href=\"#remarks\" aria-label=\"remarks permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Remarks</h4>\n<p>A lot of our work was based on the exceptional work at the <a href=\"https://poker.cs.ualberta.ca/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">University of Alberta Computer Poker Research Group</a>. Do check out their work!</p>\n<p>Our report can be found <a href=\"/poker-report.pdf\">here</a>.</p>","fields":{"slug":"/posts/poker-agent","tagSlugs":["/tag/counter-factual-regret-learning/","/tag/deep-q-learning/"]},"frontmatter":{"date":"2019-04-02","description":"Creating a poker agent based on Counter-Factual Regret Minimisation and Deep-Q Learning","tags":["counter-factual regret learning","deep-q learning"],"title":"What goes into a Poker Agent?"}}},"pageContext":{"slug":"/posts/poker-agent"}},"staticQueryHashes":["251939775","3439816877","401334301"]}